{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from fmiopendata.wfs import download_stored_query\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1ffd2",
   "metadata": {},
   "source": [
    "#### need to adjust dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc914bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.date(2023, 12, 1)  # Example start date\n",
    "end_date = dt.date(2023, 12, 31)\n",
    "\n",
    "data_list = []  # List to collect data\n",
    "\n",
    "current_date = start_date\n",
    "while tqdm(current_date <= end_date):\n",
    "    start_time = current_date.isoformat() + \"T00:00:00Z\"\n",
    "    end_time = current_date.isoformat() + \"T23:59:59Z\"\n",
    "\n",
    "    # Download data for the current day\n",
    "    obs = download_stored_query(\"fmi::observations::weather::multipointcoverage\",\n",
    "                        args=[\"bbox=18,55,35,75\",  #whole finland\n",
    "                              \"starttime=\" + start_time,\n",
    "                              \"timestep=\"+str(60*24),  #daily entries\n",
    "                              \"endtime=\" + end_time,\n",
    "                             \"timeseries=True\"])\n",
    "\n",
    "    # Parse and organize the data\n",
    "    for station, station_data in obs.data.items():\n",
    "        times = station_data['times']\n",
    "        for param, values in station_data.items():\n",
    "            if param != 'times':  # Skip the 'times' key\n",
    "                for time, value in zip(times, values['values']):\n",
    "                    data_list.append({'Timestamp': time, 'Station': station, param: value})\n",
    "                    \n",
    "    current_date += dt.timedelta(days=1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2665db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1e567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82f5a22b",
   "metadata": {},
   "source": [
    "#### data sorted by measurement type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all unique data types (excluding 'Timestamp' and 'Station')\n",
    "data_types = set(key for entry in data_list for key in entry if key not in ['Timestamp', 'Station'])\n",
    "\n",
    "# Initialize a dictionary to hold a DataFrame for each data type\n",
    "dfs = {}\n",
    "\n",
    "# Extract unique station names and timestamps\n",
    "station_names = sorted(set(entry['Station'] for entry in data_list))\n",
    "timestamps = sorted(set(entry['Timestamp'] for entry in data_list))\n",
    "\n",
    "# Create a DataFrame for each data type\n",
    "for data_type in data_types:\n",
    "    # Filter entries for the current data type\n",
    "    filtered_data = [\n",
    "        {key: value for key, value in entry.items() if key in ['Timestamp', 'Station', data_type]}\n",
    "        for entry in data_list if data_type in entry\n",
    "    ]\n",
    "\n",
    "    # Initialize an empty DataFrame for the current data type\n",
    "    df = pd.DataFrame(columns=station_names, index=pd.to_datetime(timestamps))\n",
    "\n",
    "    # Fill the DataFrame with the current data type's measurements\n",
    "    for entry in filtered_data:\n",
    "        timestamp = entry['Timestamp']\n",
    "        station = entry['Station']\n",
    "        value = entry.get(data_type)  # Use .get() to handle missing data_type in some entries\n",
    "        df.at[timestamp, station] = value\n",
    "\n",
    "    # Filter rows that have data from at least half of the measurement stations\n",
    "    threshold = len(station_names) // 2  # At least half of the stations must have data\n",
    "    df_filtered = df.dropna(thresh=threshold)\n",
    "\n",
    "    # Store the filtered DataFrame in the dictionary\n",
    "    dfs[data_type] = df_filtered\n",
    "\n",
    "# Access a specific filtered DataFrame by its data type, for example:\n",
    "dfs['Wind speed']  # For filtered wind speed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55306a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the subfolder name\n",
    "subfolder = 'data_by_measurement_type'\n",
    "\n",
    "# Check if the subfolder exists, and if not, create it\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "# Iterate over the dfs dictionary to save each DataFrame to a CSV file in the subfolder\n",
    "for data_type, df in dfs.items():\n",
    "    # Format the data_type string to create a valid and readable filename\n",
    "    filename = f\"{data_type.replace(' ', '_').lower()}_data.csv\"\n",
    "    # Create the full path by joining the subfolder and filename\n",
    "    full_path = os.path.join(subfolder, filename)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file at the full path\n",
    "    df.to_csv(full_path)\n",
    "    \n",
    "    print(f\"Saved {data_type} data to {full_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c36d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c896868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acaec842",
   "metadata": {},
   "source": [
    "#### Data grouped by the measurement station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify all unique stations and measurement types\n",
    "stations = sorted(set(entry['Station'] for entry in data_list))\n",
    "measurement_types = sorted(set(key for entry in data_list for key in entry if key not in ['Timestamp', 'Station']))\n",
    "\n",
    "# Initialize a dictionary to hold a DataFrame for each station\n",
    "station_dfs = {}\n",
    "\n",
    "# Create a DataFrame for each station\n",
    "for station in stations:\n",
    "    # Filter entries for the current station\n",
    "    station_data = [entry for entry in data_list if entry['Station'] == station]\n",
    "    \n",
    "    # Extract timestamps for the current station\n",
    "    timestamps = sorted(set(entry['Timestamp'] for entry in station_data))\n",
    "    \n",
    "    # Initialize an empty DataFrame for the current station\n",
    "    df = pd.DataFrame(index=pd.to_datetime(timestamps), columns=measurement_types)\n",
    "    \n",
    "    # Fill the DataFrame with measurements\n",
    "    for entry in station_data:\n",
    "        timestamp = entry['Timestamp']\n",
    "        for measurement in measurement_types:\n",
    "            if measurement in entry:\n",
    "                df.at[timestamp, measurement] = entry[measurement]\n",
    "\n",
    "    # Store the DataFrame in the dictionary\n",
    "    station_dfs[station] = df\n",
    "\n",
    "# Define the subfolder name\n",
    "subfolder = 'data_by_station'\n",
    "\n",
    "# Check if the subfolder exists, and if not, create it\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "# Save each station's DataFrame to a CSV file in the subfolder\n",
    "for station, df in station_dfs.items():\n",
    "    # Format the station name to create a valid and readable filename\n",
    "    filename = f\"{station.replace(' ', '_').replace('/', '_').lower()}.csv\"\n",
    "    full_path = os.path.join(subfolder, filename)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(full_path)\n",
    "    \n",
    "    print(f\"Saved data for {station} to {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84a8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28c8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643fed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
