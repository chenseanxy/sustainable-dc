{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from fmiopendata.wfs import download_stored_query\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff871e3",
   "metadata": {},
   "source": [
    "#### without retry policy, 6 days intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a4d446d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from 2023-10-01T00:00:00Z to 2023-10-07T23:59:59Z\n",
      "Downloading data from 2023-10-07T00:00:00Z to 2023-10-13T23:59:59Z\n",
      "Downloading data from 2023-10-13T00:00:00Z to 2023-10-19T23:59:59Z\n",
      "Downloading data from 2023-10-19T00:00:00Z to 2023-10-25T23:59:59Z\n",
      "Downloading data from 2023-10-25T00:00:00Z to 2023-10-31T23:59:59Z\n",
      "Downloading data from 2023-10-31T00:00:00Z to 2023-11-06T23:59:59Z\n",
      "Downloading data from 2023-11-06T00:00:00Z to 2023-11-12T23:59:59Z\n",
      "Downloading data from 2023-11-12T00:00:00Z to 2023-11-18T23:59:59Z\n",
      "Downloading data from 2023-11-18T00:00:00Z to 2023-11-24T23:59:59Z\n",
      "Downloading data from 2023-11-24T00:00:00Z to 2023-11-30T23:59:59Z\n",
      "Downloading data from 2023-11-30T00:00:00Z to 2023-12-06T23:59:59Z\n",
      "Downloading data from 2023-12-06T00:00:00Z to 2023-12-12T23:59:59Z\n",
      "Downloading data from 2023-12-12T00:00:00Z to 2023-12-18T23:59:59Z\n",
      "Downloading data from 2023-12-18T00:00:00Z to 2023-12-24T23:59:59Z\n",
      "Downloading data from 2023-12-24T00:00:00Z to 2023-12-30T23:59:59Z\n",
      "Downloading data from 2023-12-30T00:00:00Z to 2023-12-31T23:59:59Z\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from fmiopendata.wfs import download_stored_query\n",
    "\n",
    "start_date = dt.date(2023, 10, 1)  # Example start date\n",
    "end_date = dt.date(2023, 12, 31)\n",
    "\n",
    "data_list = []  # List to collect data\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Set the start time to the current date\n",
    "    start_time = current_date.isoformat() + \"T00:00:00Z\"\n",
    "    # Calculate the end time 7 days from the start time\n",
    "    end_date_interval = current_date + dt.timedelta(days=6)\n",
    "    # Make sure the end date does not exceed the overall end date\n",
    "    if end_date_interval > end_date:\n",
    "        end_date_interval = end_date\n",
    "    end_time = end_date_interval.isoformat() + \"T23:59:59Z\"\n",
    "\n",
    "    print(f\"Downloading data from {start_time} to {end_time}\")  # Optional: print statement to track progress\n",
    "\n",
    "    # Download data for the current 7-day interval\n",
    "    obs = download_stored_query(\"fmi::observations::weather::multipointcoverage\",\n",
    "                                args=[\"bbox=18,55,35,75\",  # whole Finland\n",
    "                                      \"starttime=\" + start_time,\n",
    "                                      \"timestep=\"+str(60*24),  # daily entries\n",
    "                                      \"endtime=\" + end_time,\n",
    "                                      \"timeseries=True\"])\n",
    "\n",
    "    # Parse and organize the data\n",
    "    for station, station_data in obs.data.items():\n",
    "        times = station_data['times']\n",
    "        for param, values in station_data.items():\n",
    "            if param != 'times':  # Skip the 'times' key\n",
    "                for time, value in zip(times, values['values']):\n",
    "                    data_list.append({'Timestamp': time, 'Station': station, param: value})\n",
    "\n",
    "    # Move to the next 7-day interval for the next iteration\n",
    "    current_date += dt.timedelta(days=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c977496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from fmiopendata.wfs import download_stored_query\n",
    "import time  # For adding delay between retries\n",
    "\n",
    "start_date = dt.date(2023, 10, 1)  # Example start date\n",
    "end_date = dt.date(2023, 12, 31)\n",
    "\n",
    "data_list = []  # List to collect data\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Set the start time to the current date\n",
    "    start_time = current_date.isoformat() + \"T00:00:00Z\"\n",
    "    # Calculate the end time 7 days from the start time\n",
    "    end_date_interval = current_date + dt.timedelta(days=6)\n",
    "    # Make sure the end date does not exceed the overall end date\n",
    "    if end_date_interval > end_date:\n",
    "        end_date_interval = end_date\n",
    "    end_time = end_date_interval.isoformat() + \"T23:59:59Z\"\n",
    "\n",
    "    print(f\"Downloading data from {start_time} to {end_time}\")  # Optional: print statement to track progress\n",
    "\n",
    "    max_retries = 5  # Maximum number of retries for each download attempt\n",
    "    retry_delay = 5  # Delay between retries in seconds\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Download data for the current 7-day interval\n",
    "            obs = download_stored_query(\"fmi::observations::weather::multipointcoverage\",\n",
    "                                        args=[\"bbox=18,55,35,75\",  # whole Finland\n",
    "                                              \"starttime=\" + start_time,\n",
    "                                              \"timestep=\" + str(60*24),  # daily entries\n",
    "                                              \"endtime=\" + end_time,\n",
    "                                              \"timeseries=True\"])\n",
    "            # If download is successful, break out of the retry loop\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)  # Wait before retrying\n",
    "            else:\n",
    "                print(\"Max retries reached, moving to next interval\")\n",
    "                obs = None  # Set obs to None to handle the case where all retries fail\n",
    "\n",
    "    if obs is not None:\n",
    "        # Parse and organize the data only if the download was successful\n",
    "        for station, station_data in obs.data.items():\n",
    "            times = station_data['times']\n",
    "            for param, values in station_data.items():\n",
    "                if param != 'times':  # Skip the 'times' key\n",
    "                    for time, value in zip(times, values['values']):\n",
    "                        data_list.append({'Timestamp': time, 'Station': station, param: value})\n",
    "\n",
    "    # Move to the next 7-day interval for the next iteration\n",
    "    current_date += dt.timedelta(days=7)  # Fixed to add 7 instead of 6 to avoid overlapping dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593f935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1ab2071",
   "metadata": {},
   "source": [
    "#### data sorted by measurement type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520531cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all unique data types (excluding 'Timestamp' and 'Station')\n",
    "data_types = set(key for entry in data_list for key in entry if key not in ['Timestamp', 'Station'])\n",
    "\n",
    "# Initialize a dictionary to hold a DataFrame for each data type\n",
    "dfs = {}\n",
    "\n",
    "# Extract unique station names and timestamps\n",
    "station_names = sorted(set(entry['Station'] for entry in data_list))\n",
    "timestamps = sorted(set(entry['Timestamp'] for entry in data_list))\n",
    "\n",
    "# Create a DataFrame for each data type\n",
    "for data_type in data_types:\n",
    "    # Filter entries for the current data type\n",
    "    filtered_data = [\n",
    "        {key: value for key, value in entry.items() if key in ['Timestamp', 'Station', data_type]}\n",
    "        for entry in data_list if data_type in entry\n",
    "    ]\n",
    "\n",
    "    # Initialize an empty DataFrame for the current data type\n",
    "    df = pd.DataFrame(columns=station_names, index=pd.to_datetime(timestamps))\n",
    "\n",
    "    # Fill the DataFrame with the current data type's measurements\n",
    "    for entry in filtered_data:\n",
    "        timestamp = entry['Timestamp']\n",
    "        station = entry['Station']\n",
    "        value = entry.get(data_type)  # Use .get() to handle missing data_type in some entries\n",
    "        df.at[timestamp, station] = value\n",
    "\n",
    "    # Filter rows that have data from at least half of the measurement stations\n",
    "    threshold = len(station_names) // 2  # At least half of the stations must have data\n",
    "    df_filtered = df.dropna(thresh=threshold)\n",
    "\n",
    "    # Store the filtered DataFrame in the dictionary\n",
    "    dfs[data_type] = df_filtered\n",
    "\n",
    "# Access a specific filtered DataFrame by its data type, for example:\n",
    "dfs['Wind speed']  # For filtered wind speed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693061a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the subfolder name\n",
    "subfolder = 'data_by_measurement_type'\n",
    "\n",
    "# Check if the subfolder exists, and if not, create it\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "# Iterate over the dfs dictionary to save each DataFrame to a CSV file in the subfolder\n",
    "for data_type, df in dfs.items():\n",
    "    # Format the data_type string to create a valid and readable filename\n",
    "    filename = f\"{data_type.replace(' ', '_').lower()}_data.csv\"\n",
    "    # Create the full path by joining the subfolder and filename\n",
    "    full_path = os.path.join(subfolder, filename)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file at the full path\n",
    "    df.to_csv(full_path)\n",
    "    \n",
    "    print(f\"Saved {data_type} data to {full_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c06e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819a60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6dc862",
   "metadata": {},
   "source": [
    "#### Data grouped by the measurement station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify all unique stations and measurement types\n",
    "stations = sorted(set(entry['Station'] for entry in data_list))\n",
    "measurement_types = sorted(set(key for entry in data_list for key in entry if key not in ['Timestamp', 'Station']))\n",
    "\n",
    "# Initialize a dictionary to hold a DataFrame for each station\n",
    "station_dfs = {}\n",
    "\n",
    "# Create a DataFrame for each station\n",
    "for station in stations:\n",
    "    # Filter entries for the current station\n",
    "    station_data = [entry for entry in data_list if entry['Station'] == station]\n",
    "    \n",
    "    # Extract timestamps for the current station\n",
    "    timestamps = sorted(set(entry['Timestamp'] for entry in station_data))\n",
    "    \n",
    "    # Initialize an empty DataFrame for the current station\n",
    "    df = pd.DataFrame(index=pd.to_datetime(timestamps), columns=measurement_types)\n",
    "    \n",
    "    # Fill the DataFrame with measurements\n",
    "    for entry in station_data:\n",
    "        timestamp = entry['Timestamp']\n",
    "        for measurement in measurement_types:\n",
    "            if measurement in entry:\n",
    "                df.at[timestamp, measurement] = entry[measurement]\n",
    "\n",
    "    # Store the DataFrame in the dictionary\n",
    "    station_dfs[station] = df\n",
    "\n",
    "# Define the subfolder name\n",
    "subfolder = 'data_by_station'\n",
    "\n",
    "# Check if the subfolder exists, and if not, create it\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "# Save each station's DataFrame to a CSV file in the subfolder\n",
    "for station, df in station_dfs.items():\n",
    "    # Format the station name to create a valid and readable filename\n",
    "    filename = f\"{station.replace(' ', '_').replace('/', '_').lower()}.csv\"\n",
    "    full_path = os.path.join(subfolder, filename)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(full_path)\n",
    "    \n",
    "    print(f\"Saved data for {station} to {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f108b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91884ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcef61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
